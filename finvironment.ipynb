{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-_rZVkCmmW-7",
        "outputId": "ebfd493f-94f8-4254-a128-5102a1b3a6fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.42.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m931.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.25.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.42.0-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.42.0 watchdog-6.0.0\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "added 22 packages in 7s\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!pip install streamlit\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki6vVADZh9Bd"
      },
      "source": [
        "2b901592-33ff-4bde-8d93-19cbb2fd3676\n",
        "https://www.arliai.com/docs\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlFF5bpedYH6",
        "outputId": "ba34509f-d169-4ea4-ef82-671f982a313b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing plant_tree.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile plant_tree.py\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "def plant_tree(n):\n",
        "  url = \"https://api.verdn.com/v2/pledge-transaction\"\n",
        "\n",
        "  payload = {\n",
        "      \"reference\": \"FINVIRONMENT\",\n",
        "      \"recipient\": {\"email\": \"afsharisamin1@gmail.com\"},\n",
        "      \"pledges\": [{\"impact\": {\n",
        "                  \"offeringId\": \"io_01J4RSBX6KPQ4RYCRZ8C4QWRHF\",\n",
        "                  \"amount\": n\n",
        "              }}]\n",
        "  }\n",
        "  headers = {\n",
        "      \"Authorization\": \"Bearer verdn_sk_test_2w6fgjr0onsvur4sx9lbv9x3a6f5aqzgiytbx1dc838slsm38v2zw7uzwjn\",\n",
        "      \"Content-Type\": \"application/json\"\n",
        "  }\n",
        "\n",
        "  response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
        "\n",
        "  print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2GPZSwoqKK4"
      },
      "source": [
        "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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c14-1eLWqpji",
        "outputId": "9be5a5d3-6806-4a90-b514-ad5e3f4c3995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing black_litterman.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile black_litterman.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "\n",
        "def get_user_input():\n",
        "    num_assets = 2 #int(input(\"Enter the number of stocks: \"))\n",
        "    stock_symbols = []\n",
        "\n",
        "    stock_symbols.append(\"AAPL\")\n",
        "    stock_symbols.append(\"MSFT\")\n",
        "\n",
        "    time_horizon = 0.019*2\n",
        "    return stock_symbols, time_horizon\n",
        "\n",
        "def fetch_historical_data(stock_symbols, start_date=\"2010-01-01\", end_date=\"2023-01-01\"):\n",
        "    data = yf.download(stock_symbols, start=start_date, end=end_date)['Close']\n",
        "    returns = data.pct_change().dropna()\n",
        "    return returns\n",
        "\n",
        "def calculate_expected_returns_and_cov_matrix(returns, time_horizon):\n",
        "    expected_returns = returns.mean() * time_horizon\n",
        "    cov_matrix = returns.cov() * time_horizon\n",
        "    return expected_returns, cov_matrix\n",
        "\n",
        "def calculate_market_equilibrium_returns(cov_matrix, market_caps):\n",
        "    market_weights = market_caps / np.sum(market_caps)\n",
        "    equilibrium_returns = np.dot(cov_matrix, market_weights)\n",
        "    return equilibrium_returns\n",
        "\n",
        "def adjust_returns_with_views(equilibrium_returns, cov_matrix, P, Q, omega):\n",
        "    tau = 0.025\n",
        "    M_inverse = np.linalg.inv(np.dot(np.dot(tau, P), np.dot(cov_matrix, P.T)) + omega)\n",
        "    adjusted_returns = equilibrium_returns + np.dot(np.dot(np.dot(cov_matrix, P.T), M_inverse), (Q - np.dot(P, equilibrium_returns)))\n",
        "    return adjusted_returns\n",
        "\n",
        "def portfolio_variance(weights, cov_matrix):\n",
        "    return np.dot(weights.T, np.dot(cov_matrix, weights))\n",
        "\n",
        "def portfolio_return(weights, expected_returns):\n",
        "    return np.dot(weights, expected_returns)\n",
        "\n",
        "def optimize_portfolio(expected_returns, cov_matrix, target_return):\n",
        "    num_assets = len(expected_returns)\n",
        "\n",
        "    def constraint(weights):\n",
        "        return np.sum(weights) - 1\n",
        "\n",
        "    def return_constraint(weights):\n",
        "        return portfolio_return(weights, expected_returns) - target_return\n",
        "\n",
        "    bounds = [(0, 1) for _ in range(num_assets)]\n",
        "    initial_weights = [1. / num_assets] * num_assets\n",
        "\n",
        "    result = minimize(portfolio_variance, initial_weights, args=(cov_matrix,), method='SLSQP', bounds=bounds, constraints=[{'type': 'eq', 'fun': constraint}, {'type': 'eq', 'fun': return_constraint}])\n",
        "\n",
        "    return result.x\n",
        "\n",
        "def calculate_efficient_frontier(expected_returns, cov_matrix, num_portfolios=100):\n",
        "    target_returns = np.linspace(min(expected_returns), max(expected_returns), num_portfolios)\n",
        "    frontier_returns = []\n",
        "    frontier_risks = []\n",
        "    frontier_weights = []\n",
        "\n",
        "    for target_return in target_returns:\n",
        "        weights = optimize_portfolio(expected_returns, cov_matrix, target_return)\n",
        "        frontier_returns.append(portfolio_return(weights, expected_returns))\n",
        "        frontier_risks.append(np.sqrt(portfolio_variance(weights, cov_matrix)))\n",
        "        frontier_weights.append(weights)\n",
        "\n",
        "    return frontier_returns, frontier_risks, frontier_weights\n",
        "\n",
        "def plot_efficient_frontier(frontier_returns, frontier_risks, frontier_weights, risk_free_rate=0.0106):\n",
        "    '''\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(frontier_risks, frontier_returns, c=np.array(frontier_returns)/np.array(frontier_risks), cmap='YlGnBu', marker='o')\n",
        "    plt.title('Efficient Frontier')\n",
        "    plt.xlabel('Risk (Standard Deviation)')\n",
        "    plt.ylabel('Return')\n",
        "    plt.colorbar(label='Sharpe Ratio')\n",
        "    '''\n",
        "    max_sharpe_idx = np.argmax(np.array(frontier_returns) / np.array(frontier_risks))\n",
        "    max_sharpe_ratio = frontier_returns[max_sharpe_idx] / frontier_risks[max_sharpe_idx]\n",
        "    max_sharpe_return = frontier_returns[max_sharpe_idx]\n",
        "    max_sharpe_risk = frontier_risks[max_sharpe_idx]\n",
        "\n",
        "    #plt.scatter(max_sharpe_risk, max_sharpe_return, marker='*', color='r', s=100, label='Max Sharpe Ratio')\n",
        "\n",
        "    cml_x = [0, max_sharpe_risk]\n",
        "    cml_y = [risk_free_rate, max_sharpe_return]\n",
        "    '''\n",
        "    plt.plot(cml_x, cml_y, color='r', linestyle='--', label='Capital Market Line (CML)')\n",
        "\n",
        "    plt.legend()\n",
        "    #plt.show()\n",
        "    '''\n",
        "\n",
        "    #print(\"Optimal Weights for Maximum Sharpe Ratio Portfolio:\")\n",
        "    weights = []\n",
        "    for i in range(len(frontier_weights[max_sharpe_idx])):\n",
        "        #print(f\"Stock {i+1}: {frontier_weights[max_sharpe_idx][i]:.4f}\")\n",
        "        formatted_value = round(frontier_weights[max_sharpe_idx][i], 4)\n",
        "        weights.append(formatted_value)\n",
        "    return weights\n",
        "\n",
        "def blackLitterman(p_value, q_value):\n",
        "    stock_symbols, time_horizon = get_user_input()\n",
        "\n",
        "    returns = fetch_historical_data(stock_symbols)\n",
        "    expected_returns, cov_matrix = calculate_expected_returns_and_cov_matrix(returns, time_horizon)\n",
        "\n",
        "    # Fetch market capitalizations\n",
        "    market_caps = []\n",
        "    for symbol in stock_symbols:\n",
        "        ticker = yf.Ticker(symbol)\n",
        "        market_cap = ticker.info['marketCap']\n",
        "        market_caps.append(market_cap)\n",
        "\n",
        "    equilibrium_returns = calculate_market_equilibrium_returns(cov_matrix, market_caps)\n",
        "\n",
        "    # Define P based on p_value\n",
        "    if p_value == 0:\n",
        "        P = np.array([[1, -1]])\n",
        "    elif p_value == 1:\n",
        "        P = np.array([[-1, 1]])\n",
        "    elif p_value == 2:\n",
        "        P = np.array([[1, 0]])\n",
        "    elif p_value == 3:\n",
        "        P = np.array([[0, 1]])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid p_value. Must be 0, 1, 2, or 3.\")\n",
        "\n",
        "    Q = np.array([q_value])\n",
        "\n",
        "    # Set uncertainty in views (omega) as a diagonal matrix with small values\n",
        "    omega = np.diag([0.0001])\n",
        "\n",
        "    adjusted_returns = adjust_returns_with_views(equilibrium_returns, cov_matrix, P, Q, omega)\n",
        "\n",
        "    #print(\"Expected Returns:\", adjusted_returns)\n",
        "    #print(\"Covariance Matrix:\\n\", cov_matrix)\n",
        "\n",
        "    frontier_returns, frontier_risks, frontier_weights = calculate_efficient_frontier(adjusted_returns, cov_matrix)\n",
        "\n",
        "    return plot_efficient_frontier(frontier_returns, frontier_risks, frontier_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vTlsH1MSTd0-",
        "outputId": "68a845e0-61a9-49e6-a9a6-4319f384cd3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.42.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.25.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZGP8pACC-TI",
        "outputId": "f88a8eaf-35d8-41f5-b6e8-c9a6b209861c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "\n",
        "\n",
        "jobdescription = \"You are a portfolio manager and are looking at the changes to a portfolio made from anlysing the sentiment of current news aricles.\"\n",
        "message_1 =\"Hello there! Why have I lost money?\"\n",
        "\n",
        "import sys\n",
        "# Append the directory to your python path using sys\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "# Import the module\n",
        "from plant_tree import plant_tree\n",
        "from black_litterman import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import csv\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "\n",
        "API_KEY = \"Secret_key\"\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 768  # BERT embedding dimension\n",
        "HIDDEN_DIM = 256\n",
        "ACTION_SPACE = 5\n",
        "LEARNING_RATE = 1e-4\n",
        "GAMMA = 0.99  # Discount factor for rewards\n",
        "BATCH_SIZE = 10  # Number of episodes before updating the policy\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transformer-based Policy Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class TransformerPolicy(nn.Module):\n",
        "    def __init__(self, action_space, hidden_dim=1024, num_layers=24, num_heads=16, use_residual=True, dropout_rate=0.1):\n",
        "        super(TransformerPolicy, self).__init__()\n",
        "        # Load pre-trained BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Define a much deeper and more complex network\n",
        "        self.use_residual = use_residual\n",
        "        self.layers = nn.ModuleList()\n",
        "        input_dim = self.bert.config.hidden_size  # BERT's hidden size (768 for bert-base-uncased)\n",
        "\n",
        "        # Initial projection to hidden_dim\n",
        "        self.projection = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Multi-head self-attention blocks\n",
        "        self.attention_blocks = nn.ModuleList()\n",
        "        for _ in range(num_layers // 2):  # Add attention blocks every 2 layers\n",
        "            self.attention_blocks.append(\n",
        "                nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, dropout=dropout_rate)\n",
        "            )\n",
        "\n",
        "        # Deep feed-forward network\n",
        "        for i in range(num_layers):\n",
        "            # Add a linear layer with intermediate expansion\n",
        "            self.layers.append(nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim * 4),  # Expand dimension\n",
        "                nn.GELU(),\n",
        "                nn.Linear(hidden_dim * 4, hidden_dim),  # Project back to hidden_dim\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ))\n",
        "            # Add Layer Normalization (pre-norm)\n",
        "            self.layers.append(nn.LayerNorm(hidden_dim))\n",
        "\n",
        "        # Final layer to map to action space\n",
        "        self.final_layer = nn.Linear(hidden_dim, action_space)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get BERT embeddings\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output  # Use the [CLS] token representation\n",
        "\n",
        "        # Project BERT output to hidden_dim\n",
        "        x = self.projection(pooled_output)\n",
        "\n",
        "        # Pass through the deep network with attention blocks\n",
        "        attention_block_idx = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, nn.Sequential):  # Feed-forward block\n",
        "                residual = x if self.use_residual else 0\n",
        "                x = layer(x)\n",
        "                if self.use_residual:\n",
        "                    x = x + residual\n",
        "            elif isinstance(layer, nn.LayerNorm):  # Layer normalization\n",
        "                x = layer(x)\n",
        "\n",
        "            # Insert multi-head self-attention every 2 layers\n",
        "            if i % 2 == 0 and attention_block_idx < len(self.attention_blocks):\n",
        "                x = x.unsqueeze(0)  # Add batch dimension for attention\n",
        "                x, _ = self.attention_blocks[attention_block_idx](x, x, x)\n",
        "                x = x.squeeze(0)  # Remove batch dimension\n",
        "                attention_block_idx += 1\n",
        "\n",
        "        # Final layer to produce logits\n",
        "        logits = self.final_layer(x)\n",
        "        logits = torch.clamp(logits, min=-10, max=10)  # Clip logits to avoid extreme values\n",
        "        action_probs = self.softmax(logits + 1e-9)  # Add epsilon for numerical stability\n",
        "        return action_probs\n",
        "\n",
        "\n",
        "# Reinforcement Learning Agent\n",
        "class RLAgent:\n",
        "    def __init__(self, policy_net, optimizer, gamma):\n",
        "        self.policy_net = policy_net\n",
        "        self.optimizer = optimizer\n",
        "        self.gamma = gamma\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "        input_ids = state['input_ids'].to(device)\n",
        "        attention_mask = state['attention_mask'].to(device)\n",
        "\n",
        "        # Get action probabilities\n",
        "        action_probs = self.policy_net(input_ids, attention_mask)\n",
        "\n",
        "        # Clone before modification to avoid in-place changes\n",
        "        modified_probs = action_probs.clone()\n",
        "\n",
        "        # Define the excluded action\n",
        "        excluded_action = 0\n",
        "\n",
        "        # Store the probability of the excluded action\n",
        "        excluded_action_prob = modified_probs[0, excluded_action].item()\n",
        "\n",
        "        # Set the probability of the excluded action to 0\n",
        "        modified_probs[0, excluded_action] = 0.0\n",
        "\n",
        "        # Normalize the remaining probabilities\n",
        "        modified_probs = modified_probs / modified_probs.sum()\n",
        "\n",
        "        # Sample an action from the modified probability distribution\n",
        "        m = Categorical(modified_probs)\n",
        "        action = m.sample()\n",
        "\n",
        "        # Save log probability for training\n",
        "        self.saved_log_probs.append(m.log_prob(action))\n",
        "\n",
        "        return action.item(), excluded_action_prob\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update_policy(self):\n",
        "        R = 0\n",
        "        policy_loss = []\n",
        "        returns = []\n",
        "\n",
        "        # Calculate discounted returns\n",
        "        for r in self.rewards[::-1]:\n",
        "            R = r + self.gamma * R\n",
        "            returns.insert(0, R)\n",
        "\n",
        "        returns = torch.tensor(returns).to(device)\n",
        "        if len(returns) > 1:  # Only normalize if there are multiple returns\n",
        "            returns = (returns - returns.mean()) / (returns.std() + 1e-9)  # Normalize returns\n",
        "\n",
        "        # Calculate policy loss\n",
        "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * R)\n",
        "\n",
        "        # Optimize the policy\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Clear saved rewards and log probabilities\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "# Tokenizer for text input\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Initialize policy network and optimizer\n",
        "policy_net = TransformerPolicy(ACTION_SPACE).to(device)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "agent = RLAgent(policy_net, optimizer, GAMMA)\n",
        "\n",
        "def load_data():\n",
        "  df1 = pd.read_csv('AppleNewsStock.csv')\n",
        "  df2 = pd.read_csv('MicrosoftNewsStock.csv')\n",
        "  return df1, df2\n",
        "\n",
        "def process_data(df1, df2):\n",
        "  processed_data = []\n",
        "  days = 7\n",
        "  data_chunk = \"\"\n",
        "  for (index1, row1), (index2, row2) in zip(df1.iterrows(), df2.iterrows()):\n",
        "    if days>0:\n",
        "      data_chunk += str(row1['Open'])+str(row1['High'])+str(row1['Low'])+str(row1['Close'])+str(row1['Adj Close'])+str(row1['Volume'])+str(row1['News'])\n",
        "      data_chunk += str(row2['Open'])+str(row2['High'])+str(row2['Low'])+str(row2['Close'])+str(row2['Adj Close'])+str(row2['Volume'])+str(row2['News'])\n",
        "      days-=1\n",
        "    else:\n",
        "      days = 7\n",
        "      processed_data.append(data_chunk)\n",
        "      data_chunk = \"\"\n",
        "  return processed_data\n",
        "\n",
        "def get_news_for_time_period(timestamp):\n",
        "    \"\"\"Retrieve news articles corresponding to a given time period.\"\"\"\n",
        "    df1,df2 = load_data()\n",
        "    filtered_df = df1[df1['Date'] == timestamp]\n",
        "    if not filtered_df.empty:\n",
        "        return filtered_df.iloc[0]['News']\n",
        "    return None\n",
        "def generate_chatgpt_explanation(p, q, timestamp):\n",
        "    news_articles = get_news_for_time_period(timestamp)\n",
        "    if p == 0:\n",
        "        P = np.array([[1, -1]])\n",
        "    elif p == 1:\n",
        "        P = np.array([[-1, 1]])\n",
        "    elif p == 2:\n",
        "        P = np.array([[1, 0]])\n",
        "    elif p == 3:\n",
        "        P = np.array([[0, 1]])\n",
        "\n",
        "    \"\"\"Generate an explanation for why P and Q values were chosen based on news context.\"\"\"\n",
        "    prompt = f\"\"\" The 0th index in the p value is the apple's stock and the 1st index being microsoft stock.\n",
        "    p values can be of 4 types, [1,-1] which means the price of 0th index is going to outperform the price of 1st index by the amount given in the Q value, [-1,1] means 1st index is going to outperform 2nd index by it's q value,[1,0] means price of 0th index is going to rise by its q value and [0,1] price of 1st index is going to rise by its q value\n",
        "    The following are the P and Q values predicted by a reinforcement learning model:\n",
        "    - P value: {p}\n",
        "    - Q value: {q}\n",
        "\n",
        "    The corresponding news articles for this time period are:\n",
        "    {news_articles}\n",
        "\n",
        "    Based on the news context, explain why these P and Q values were chosen by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    #return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a experienced stock portfolio manager.\"},\n",
        "            {\n",
        "                \"role\": \"user\", \"content\": prompt\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return(completion.choices[0].message.content)\n",
        "\n",
        "\n",
        "# Example training loop\n",
        "def train(episodes):\n",
        "    df1, df2 = load_data()\n",
        "    processed_data = process_data(df1, df2)\n",
        "    #return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "def main():\n",
        "    x = []\n",
        "    y = []\n",
        "    st.title(\"AI Based Black Litterman Portfolio Optimisation\")\n",
        "    image_url = \"https://easydrawingguides.com/wp-content/uploads/2017/02/How-to-draw-a-cartoon-tree-20.png\"\n",
        "    image_placeholder = st.empty()\n",
        "\n",
        "    # Load and process data\n",
        "    df1, df2 = load_data()\n",
        "    processed_data = process_data(df1, df2)\n",
        "\n",
        "    # Initialize policy network and RL agent\n",
        "    policy_net = TransformerPolicy(ACTION_SPACE).to(device)\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "    agent = RLAgent(policy_net, optimizer, GAMMA)\n",
        "\n",
        "    episodes = 300-240\n",
        "    balance=10000\n",
        "    trees_planted = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "      if episode%7==0:\n",
        "        text = processed_data[episode]\n",
        "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "        # Select action\n",
        "        action, excluded_action_prob = agent.select_action(inputs)\n",
        "        explanation = generate_chatgpt_explanation(p = action, q = excluded_action_prob,timestamp = df1.iloc[episode].loc['Date'])\n",
        "        st.write(f\"Episode {episode + 1}: Selected Action: {action}, Q-value: {excluded_action_prob}\")\n",
        "        aapl, msft= blackLitterman(action-1, excluded_action_prob)\n",
        "\n",
        "\n",
        "        st.write(f\"The weights for the first week's optimised portfolio: Apple: {aapl}, Microsoft: {msft}\")\n",
        "\n",
        "        aapl_in_pounds = balance*aapl\n",
        "        msft_in_pounds = balance*msft\n",
        "\n",
        "        st.write(f\"Apple: {df1.iloc[episode].loc['Date']}\")\n",
        "        st.write(f\"Microsoft: {df2.iloc[episode].loc['Date']}\")\n",
        "\n",
        "        aapl_shares = aapl_in_pounds/df1.iloc[episode].loc['Open']\n",
        "        msft_shares = msft_in_pounds/df2.iloc[episode].loc['Open']\n",
        "\n",
        "\n",
        "\n",
        "        aapl_sell_price = aapl_shares * (df1.iloc[episode+3].loc['Open'])\n",
        "        msft_sell_price = msft_shares * (df2.iloc[episode+3].loc['Open'])\n",
        "\n",
        "        if \"show_image\" not in st.session_state:\n",
        "          st.session_state[\"show_image\"] = True\n",
        "\n",
        "        if st.session_state[\"show_image\"]:\n",
        "          image_placeholder.image(image_url, use_column_width=True)\n",
        "        else:\n",
        "          image_placeholder.empty()\n",
        "\n",
        "        aapl_profit = aapl_sell_price-aapl_in_pounds\n",
        "        msft_profit = msft_sell_price-msft_in_pounds\n",
        "        total_profit = msft_profit+aapl_profit\n",
        "        if total_profit<0:\n",
        "          total_profit*=-1\n",
        "        st.write(f\"Total Profit: {total_profit}\")\n",
        "        x.append(episode)\n",
        "        n = total_profit//0.3\n",
        "        if n<0:\n",
        "          n=0\n",
        "        for i in range(int(n)):\n",
        "          st.session_state[\"show_image\"] = not st.session_state[\"show_image\"]\n",
        "        trees_planted+=n\n",
        "        plant_tree(n)\n",
        "        st.write(f\"Trees planted: {n}\")\n",
        "        balance += total_profit\n",
        "        y.append(balance)\n",
        "    st.write(f\"End Balance: {balance}\")\n",
        "    st.write(f\"Total trees planted: {trees_planted}\")\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x, y, label=\"\")\n",
        "    ax.set_xlabel(\"Episode\")\n",
        "    ax.set_ylabel(\"Profit\")\n",
        "    ax.set_title(\"Balance Line Chart\")\n",
        "    ax.legend()\n",
        "\n",
        "    # Display in Streamlit\n",
        "    st.pyplot(fig)\n",
        "    # Streamlit Sidebar UI\n",
        "    with st.sidebar:\n",
        "        st.write(\"AI-based Stock Portfolio Optimization\")\n",
        "\n",
        "        # Sample explanation generation\n",
        "        timestamp = df1.iloc[0]['Date']  # Example: Taking the first date from dataset\n",
        "        p, q = 2, 0.05  # Example P and Q values\n",
        "        st.write(\"AI Explanation for P and Q Values:\")\n",
        "        st.write(explanation)\n",
        "\n",
        "\n",
        "    st.write(\"Portfolio Optimization Completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24LAXY4PmtHK",
        "outputId": "b5dbc497-8b3e-45dc-cd9a-a67b3745f4b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.170.94.4\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0Kyour url is: https://new-mugs-stay.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &\n",
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWsWpMdtwKgz"
      },
      "source": [
        "https://api.gdeltproject.org/api/v2/doc/doc?query=AAPL&mode=artlist"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
